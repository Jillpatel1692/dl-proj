# -*- coding: utf-8 -*-
"""dl fproj.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18UxFETp7VQBXhGX9VMfidiRoEk2ZJl1X
"""

# Import necessary libraries
import numpy as np  # NumPy for numerical operations
import pandas as pd  # Pandas for data manipulation and analysis
from sklearn.feature_extraction.text import TfidfVectorizer  # TF-IDF Vectorizer for text feature extraction
from sklearn.model_selection import train_test_split  # Splitting the data into training and testing sets
from sklearn.naive_bayes import MultinomialNB  # Multinomial Naive Bayes classifier
from sklearn.metrics import classification_report  # Reporting classification metrics
from nltk.corpus import reuters  # NLTK Reuters corpus for text data
from nltk.corpus import stopwords  # NLTK stopwords for filtering common words
from nltk.tokenize import word_tokenize  # NLTK word tokenizer for text tokenization
from nltk.stem import PorterStemmer  # NLTK Porter Stemmer for word stemming

# Download NLTK resources (if not already downloaded)
import nltk
nltk.download("reuters")  # Download the Reuters corpus from NLTK
nltk.download("stopwords")  # Download NLTK stopwords
nltk.download("punkt")  # Download NLTK Punkt tokenizer for word tokenization

# Load Reuters-21578 dataset
documents = reuters.fileids()  # Get the file IDs of documents in the Reuters corpus
categories = reuters.categories()  # Get the list of categories in the Reuters corpus

# Define a function to preprocess and tokenize the documents
def preprocess_and_tokenize(document):
    stop_words = set(stopwords.words("english"))  # Get English stopwords from NLTK
    stemmer = PorterStemmer()  # Create a Porter Stemmer instance from NLTK
    words = word_tokenize(document)  # Tokenize the document using NLTK word tokenizer
    words = [stemmer.stem(word.lower()) for word in words if word.isalnum()]  # Stem and convert to lowercase
    words = [word for word in words if word not in stop_words]  # Remove stopwords
    return " ".join(words)  # Join the processed words into a string

# Preprocess and tokenize the documents
data = []
target = []
for doc_id in documents:
    category = reuters.categories(doc_id)[0]  # Get the category of the document
    document = reuters.raw(doc_id)  # Get the raw text of the document
    processed_doc = preprocess_and_tokenize(document)  # Preprocess and tokenize the document
    data.append(processed_doc)
    target.append(category)

# Convert data and target into a DataFrame
df = pd.DataFrame({'data': data, 'target': target})

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df['data'], df['target'], test_size=0.2, random_state=42)

# Vectorize the text data using TF-IDF
vectorizer = TfidfVectorizer()  # Create a TF-IDF vectorizer instance
X_train_tfidf = vectorizer.fit_transform(X_train)  # Fit and transform the training data
X_test_tfidf = vectorizer.transform(X_test)  # Transform the test data using the fitted vectorizer

# Train a Multinomial Naive Bayes classifier
classifier = MultinomialNB()  # Create a Multinomial Naive Bayes classifier instance
classifier.fit(X_train_tfidf, y_train)  # Train the classifier using the TF-IDF transformed training data

# Predict categories on the test set
y_pred = classifier.predict(X_test_tfidf)

# Evaluate the classifier
report = classification_report(y_test, y_pred)  # Generate a classification report
print(report)  # Print the classification report to the console

# Import necessary library
import numpy as np  # NumPy for numerical operations

# Define the proportions for splitting the data
train_ratio = 0.7  # Set the proportion for the training set
test_ratio = 0.15  # Set the proportion for the test set

# Shuffle the data
df_shuffled = df.sample(frac=1, random_state=42)  # Shuffle the DataFrame randomly with a fixed seed

# Split the data into three categories: training set, test set, and unused set
train_size = int(train_ratio * len(df_shuffled))  # Calculate the size of the training set
test_size = int(test_ratio * len(df_shuffled))  # Calculate the size of the test set
unused_size = len(df_shuffled) - train_size - test_size  # Calculate the size of the unused set

# Assign IDs and group names
df_shuffled['id'] = range(len(df_shuffled))  # Assign unique IDs to each row
df_shuffled['group'] = np.where(df_shuffled.index < train_size, 'training_set', np.where(df_shuffled.index < train_size + test_size, 'test_set', 'unused_set'))  # Assign group names based on index

# Separate the data into the three categories
training_set = df_shuffled[df_shuffled['group'] == 'training_set']  # Extract rows belonging to the training set
test_set = df_shuffled[df_shuffled['group'] == 'test_set']  # Extract rows belonging to the test set
unused_set = df_shuffled[df_shuffled['group'] == 'unused_set']  # Extract rows belonging to the unused set

# Now, you have three separate DataFrames: training_set, test_set, and unused_set
# Each DataFrame contains data, target, id, and group columns
print("Training Set:")
print(training_set)

print("\nTest Set:")
print(test_set)

print("\nUnused Set:")
print(unused_set)

pip install gensim

import gensim
from gensim import corpora
import nltk
nltk.download("reuters")

# Your data preprocessing code here

# Tokenize your data (assuming 'data' is your preprocessed text data)
tokenized_data = [document.split() for document in df['data']]
# Split each preprocessed document into a list of tokens (words) and store them in 'tokenized_data'

# Create a dictionary and a corpus
dictionary = corpora.Dictionary(tokenized_data)
# Create a Gensim Dictionary, which maps each unique token to a unique integer ID
corpus = [dictionary.doc2bow(text) for text in tokenized_data]
# Create a Gensim Corpus, which represents each document as a bag-of-words using the token IDs from the dictionary

# Define a function to run LDA with different numbers of topics
def run_lda(num_topics):
    lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)
    return lda_model

# Try different numbers of topics
num_topics_list = [5, 10, 15, 20]

for num_topics in num_topics_list:
    print(f"Number of Topics: {num_topics}")  # Print the number of topics being evaluated
    lda_model = run_lda(num_topics)  # Run LDA with the specified number of topics
    topics = lda_model.print_topics(num_words=10)  # Get the top 10 words for each topic
    for topic in topics:
        print(topic)  # Print the words associated with each topic
    print("\n")  # Print a newline for better readability

# Import necessary libraries
import gensim  # Gensim for topic modeling
from gensim import corpora  # Gensim's corpora module for creating a dictionary and a corpus
import nltk  # Natural Language Toolkit for text processing
import pandas as pd  # Pandas for data manipulation and analysis
import numpy as np  # NumPy for numerical operations

# Your data preprocessing code here (assumed to be present but not provided)

# Tokenize your data (assuming 'data' is your preprocessed text data)
tokenized_data = [document.split() for document in df['data']]
# Split each preprocessed document into a list of tokens (words) and store them in 'tokenized_data'

# Create a dictionary and a corpus
dictionary = corpora.Dictionary(tokenized_data)
# Create a Gensim Dictionary, which maps each unique token to a unique integer ID
corpus = [dictionary.doc2bow(text) for text in tokenized_data]
# Create a Gensim Corpus, which represents each document as a bag-of-words using the token IDs from the dictionary

# Define a function to run LDA with different numbers of topics
def run_lda(num_topics):
    lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)
    # Run LDA with the specified number of topics, using the dictionary and corpus created earlier
    return lda_model

# Try different numbers of topics
num_topics_list = [5]

for num_topics in num_topics_list:
    print(f"Number of Topics: {num_topics}")  # Print the number of topics being evaluated
    lda_model = run_lda(num_topics)  # Run LDA with the specified number of topics

    # Assign topics to documents
    topics_for_documents = [lda_model[document] for document in corpus]
    # Get the topics and their probabilities for each document in the corpus

    # Categorize documents into sets (training, test, unused)
    training_set_topics = [topics_for_documents[i] for i in training_set['id']]
    # Extract topics for documents in the training set
    test_set_topics = [topics_for_documents[i] for i in test_set['id']]
    # Extract topics for documents in the test set
    unused_set_topics = [topics_for_documents[i] for i in unused_set['id']]
    # Extract topics for documents in the unused set

    # Print the categorization for the training set
    print("Training Set Topics:")
    for i, topic_probabilities in enumerate(training_set_topics):
        max_topic = max(topic_probabilities, key=lambda x: x[1])
        print(f"Document {i} - Topic {max_topic[0]} with Probability {max_topic[1]}")
        # Print the document ID, assigned topic, and the probability of that topic

    # Print the categorization for the test set
    print("Test Set Topics:")
    for i, topic_probabilities in enumerate(test_set_topics):
        max_topic = max(topic_probabilities, key=lambda x: x[1])
        print(f"Document {i} - Topic {max_topic[0]} with Probability {max_topic[1]}")
        # Print the document ID, assigned topic, and the probability of that topic

    # Print the categorization for the unused set
    print("Unused Set Topics:")
    for i, topic_probabilities in enumerate(unused_set_topics):
        max_topic = max(topic_probabilities, key=lambda x: x[1])
        print(f"Document {i} - Topic {max_topic[0]} with Probability {max_topic[1]}")
        # Print the document ID, assigned topic, and the probability of that topic

    print("\n")  # Print a newline for better readability

"""optimality number of categories  using topic modelling with categories and true cateriogized"""

# Import necessary library
from gensim.models import CoherenceModel

# Calculate coherence scores for different numbers of topics
coherence_scores = []  # Initialize an empty list to store coherence scores
for num_topics in num_topics_list:
    lda_model = run_lda(num_topics)  # Run LDA with the specified number of topics
    coherence_model_lda = CoherenceModel(model=lda_model, texts=tokenized_data, dictionary=dictionary, coherence='c_v')
    # Create a CoherenceModel instance for the LDA model using 'c_v' coherence
    coherence_score = coherence_model_lda.get_coherence()  # Calculate the coherence score
    coherence_scores.append(coherence_score)  # Append the coherence score to the list
    print(f"Number of Topics: {num_topics}, Coherence Score: {coherence_score}")
    # Print the number of topics and its corresponding coherence score

# Find the optimal number of topics based on coherence score
optimal_num_topics = num_topics_list[np.argmax(coherence_scores)]  # Get the number of topics with the highest coherence score
print(f"Optimal Number of Topics: {optimal_num_topics}")
# Print the optimal number of topics based on the highest coherence score

# Import necessary library
from gensim.models import CoherenceModel  # Import the CoherenceModel class from Gensim

import numpy as np  # Import NumPy for numerical operations


# Your code for calculating coherence scores

# Define the top-level categories
top_level_categories = ["earn", "acq", "money-fx", "grain", "crude", "trade", "interest", "ship"]

# Print the optimal number of topics
print(f"Optimal Number of Topics: {optimal_num_topics}")

# Print the top-level categories
print("Top-Level Categories:")
for i, category in enumerate(top_level_categories):
    print(f"{i+1}. {category}")

# Import necessary libraries
import gensim  # Gensim for topic modeling
from gensim import corpora  # Gensim's corpora module for creating a dictionary and a corpus
import pandas as pd  # Pandas for data manipulation and analysis
from collections import defaultdict  # Import defaultdict from the collections module

# Your data preprocessing code here (assumed to be present but not provided)

# Tokenize your data (assuming 'data' is your preprocessed text data)
tokenized_data = [document.split() for document in df['data']]
# Split each preprocessed document into a list of tokens (words) and store them in 'tokenized_data'

# Create a dictionary and a corpus
dictionary = corpora.Dictionary(tokenized_data)
# Create a Gensim Dictionary, which maps each unique token to a unique integer ID
corpus = [dictionary.doc2bow(text) for text in tokenized_data]
# Create a Gensim Corpus, which represents each document as a bag-of-words using the token IDs from the dictionary

# Define a function to run LDA with a specific number of topics
def run_lda(num_topics):
    lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)
    # Run LDA with the specified number of topics, using the dictionary and corpus created earlier
    return lda_model

# Number of topics
num_topics = 5  # Specify the number of topics for LDA

# Run LDA with 5 topics
lda_model = run_lda(num_topics)  # Run LDA with the specified number of topics

# Initialize a list to store categories
categories_list = []  # Create an empty list to store categories

# Calculate the word count for each category across the entire corpus
for category in categories:  # Iterate through each category (replace 'categories' with your actual category data)
    category_docs = df[df['target'] == category]  # Filter documents belonging to the current category

    if len(category_docs) > 0:  # Check if there are documents in the category
        categories_list.append(category)  # Append the category to the list if it has documents

# Print the categories
print("Categories:")
for category in categories_list:
    print(f"{category}")  # Print each category in the list